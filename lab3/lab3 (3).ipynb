{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Lab 3: Intro to CNNs and DNNs\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Build and train a deep conv net\n",
    "* Explore and implement various initialization techniques\n",
    "* Implement a parameterized module in Pytorch\n",
    "* Use a principled loss function\n",
    "\n",
    "## Video Tutorial\n",
    "[https://youtu.be/3TAuTcx-VCc](https://youtu.be/3TAuTcx-VCc)\n",
    "\n",
    "## Deliverable\n",
    "For this lab, you will submit an ipython notebook via learningsuite.\n",
    "This is where you build your first deep neural network!\n",
    "\n",
    "For this lab, we'll be combining several different concepts that we've covered during class,\n",
    "including new layer types, initialization strategies, and an understanding of convolutions.\n",
    "\n",
    "## Grading Standards:\n",
    "* 20% Part 0: Successfully followed lab video and typed in code\n",
    "* 20% Part 1: Re-implement Conv2D and CrossEntropy loss function\n",
    "* 20% Part 2: Implement different initialization strategies\n",
    "* 10% Part 3: Print parameters, plot train/test accuracy\n",
    "* 10% Reach 85% validation accuracy from parts 1-3\n",
    "* 10% Part 4: Convolution parameters quiz\n",
    "* 10% Tidy and legible figures, including labeled axes where appropriate\n",
    "___\n",
    "\n",
    "### Part 0\n",
    "Watch and follow video tutorial:\n",
    "\n",
    "[https://youtu.be/3TAuTcx-VCc](https://youtu.be/3TAuTcx-VCc)\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Watch tutorial\n",
    "\n",
    "**DONE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wQOefmcZVgTl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, utils, datasets\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parameter import Parameter\n",
    "import pdb\n",
    "\n",
    "assert torch.backends.mps.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Il_53HLSWPTY"
   },
   "outputs": [],
   "source": [
    "# Use the dataset class you created in lab2\n",
    "class FashionMNISTProcessedDataset(Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.data = datasets.FashionMNIST(root, train=train, transform=transforms.ToTensor(), download=True)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.data[i]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QY4owfQwm-Ni"
   },
   "source": [
    "___\n",
    "\n",
    "### Part 1\n",
    "Re-implement a Conv2D module with parameters and a CrossEntropy loss function.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* CrossEntropyLoss \n",
    "* Conv2D\n",
    "\n",
    "**DONE:**\n",
    "\n",
    "___\n",
    "\n",
    "### Part 2\n",
    "Implement a few initialization strategies which can include Xe initialization\n",
    "(sometimes called Xavier), Orthogonal initialization, and uniform random.\n",
    "You can specify which strategy you want to use with a parameter. \n",
    "\n",
    "\n",
    "\n",
    "Helpful links include:\n",
    "*  [Orthogonal Initialization](https://hjweide.github.io/orthogonal-initialization-in-convolutional-layers) (or the original paper: http://arxiv.org/abs/1312.6120)\n",
    "*  http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "\n",
    "**TODO:**\n",
    "* Parameterize custom Conv2D for different initilization strategies\n",
    "* Xe\n",
    "* Orthogonal\n",
    "* Uniform\n",
    "\n",
    "**DONE:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2443, 3.4256, 2.6980])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.from_numpy(np.random.randn(3,10).astype(np.float32))\n",
    "b = torch.exp(a)\n",
    "i = torch.tensor([7,8,9])\n",
    "r = torch.arange(b.size(0))\n",
    "c = i\n",
    "c = c.to(\"cpu\")\n",
    "b[r,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([7, 8, 9]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.8204,  0.7309,  0.2606,  1.1834,  2.5690,  5.5925,  0.7306,  0.2443,\n",
       "          1.9174,  0.8329],\n",
       "        [ 0.9049,  1.3221,  0.7915,  0.8517,  0.2739,  1.4884,  1.7747,  0.4180,\n",
       "          3.4256,  0.7514],\n",
       "        [ 3.5522,  1.9014,  0.5553,  0.6039,  1.0626,  0.6773, 11.4739,  0.4043,\n",
       "          0.7381,  2.6980]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you pass in a tensor or a list of indices in the form $\\verb+b[r,c]+$ where $r$ and $c$ are tensors of indices, then it will index with $r$ as the rows you want to look at, and $c$ as the columns you want to look at and choose the corresponding elements. So, in the example above, if we have $r$ and $c$ as defined, the elements of the $b$ matrix we will be looking at are $(0,7), (1,8), \\text{ and }(2,9)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3794e-37, 1.5757e-36, 1.1602e-15,\n",
       "         0.0000e+00, 1.0000e+00, 2.3102e-28, 3.6951e-14],\n",
       "        [3.1730e-13, 0.0000e+00, 9.3294e-12, 3.8734e-13, 1.3607e-17, 1.0478e-03,\n",
       "         5.5936e-13, 4.5145e-13, 9.9895e-01, 1.9576e-12],\n",
       "        [0.0000e+00, 0.0000e+00, 3.5145e-28, 0.0000e+00, 9.5574e-27, 4.0868e-22,\n",
       "         0.0000e+00, 2.6799e-16, 2.0034e-34, 1.0000e+00],\n",
       "        [1.8896e-20, 1.0000e+00, 0.0000e+00, 7.7824e-13, 6.9604e-29, 0.0000e+00,\n",
       "         1.5235e-22, 1.8787e-38, 0.0000e+00, 7.9691e-28],\n",
       "        [2.2058e-35, 8.2083e-22, 2.4448e-11, 7.4304e-27, 1.0000e+00, 0.0000e+00,\n",
       "         2.5495e-10, 0.0000e+00, 8.8845e-16, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5157e-16,\n",
       "         0.0000e+00, 3.3108e-23, 8.8133e-27, 1.0000e+00],\n",
       "        [1.0000e+00, 7.5837e-38, 6.0599e-14, 5.5334e-20, 2.9965e-24, 0.0000e+00,\n",
       "         1.6991e-11, 0.0000e+00, 1.0855e-37, 6.3667e-32],\n",
       "        [9.9998e-01, 3.8310e-32, 5.9836e-14, 4.4157e-06, 2.4965e-25, 0.0000e+00,\n",
       "         1.3232e-05, 0.0000e+00, 1.9025e-33, 8.1753e-25],\n",
       "        [0.0000e+00, 1.9382e-34, 1.5411e-17, 5.5224e-25, 3.3617e-21, 7.7337e-06,\n",
       "         1.2698e-24, 1.6959e-27, 2.4772e-20, 9.9999e-01],\n",
       "        [1.4231e-31, 1.1993e-37, 9.3392e-19, 5.3213e-35, 1.1790e-22, 3.1090e-01,\n",
       "         0.0000e+00, 2.2684e-14, 1.4974e-08, 6.8910e-01],\n",
       "        [8.4942e-32, 1.0705e-30, 8.5061e-30, 2.7481e-14, 3.3143e-28, 3.6957e-03,\n",
       "         6.2853e-28, 9.9630e-01, 1.2505e-19, 4.1175e-34],\n",
       "        [9.3128e-05, 3.2525e-20, 2.8508e-04, 4.5277e-14, 9.9960e-01, 7.4863e-22,\n",
       "         2.3028e-05, 1.7682e-27, 1.6314e-12, 0.0000e+00],\n",
       "        [9.3410e-28, 0.0000e+00, 5.1997e-25, 7.3057e-26, 1.2647e-32, 9.8128e-01,\n",
       "         5.8983e-23, 1.8717e-02, 7.9417e-08, 9.8984e-16],\n",
       "        [2.6840e-28, 1.0000e+00, 6.1018e-35, 1.2185e-15, 1.3866e-19, 1.5861e-36,\n",
       "         3.4568e-24, 5.9132e-35, 4.4928e-24, 9.3468e-30],\n",
       "        [2.3761e-22, 5.7625e-32, 1.0000e+00, 2.3944e-11, 2.8803e-15, 0.0000e+00,\n",
       "         5.2424e-21, 0.0000e+00, 8.9472e-24, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.2162e-31, 0.0000e+00, 0.0000e+00, 6.7220e-18,\n",
       "         0.0000e+00, 2.8837e-14, 1.7444e-25, 1.0000e+00],\n",
       "        [2.9256e-37, 5.9833e-20, 8.9429e-18, 3.0804e-34, 1.0000e+00, 0.0000e+00,\n",
       "         2.1103e-17, 0.0000e+00, 5.7707e-37, 0.0000e+00],\n",
       "        [4.5179e-17, 1.2159e-21, 9.0725e-24, 1.4150e-12, 1.0000e+00, 0.0000e+00,\n",
       "         2.4858e-13, 0.0000e+00, 1.7820e-17, 0.0000e+00],\n",
       "        [3.3309e-28, 9.7193e-25, 4.2898e-15, 1.0672e-21, 1.0000e+00, 6.2592e-38,\n",
       "         5.3071e-15, 0.0000e+00, 5.9668e-13, 0.0000e+00],\n",
       "        [1.0446e-36, 0.0000e+00, 1.0243e-37, 1.5622e-21, 1.4567e-20, 1.0000e+00,\n",
       "         8.0854e-36, 3.3715e-06, 1.0027e-13, 2.2063e-16],\n",
       "        [1.4910e-37, 1.6710e-29, 1.4389e-34, 1.7826e-34, 5.2165e-23, 9.9997e-01,\n",
       "         1.8240e-30, 3.4737e-05, 7.5852e-09, 1.4920e-25],\n",
       "        [1.0707e-35, 2.7304e-17, 1.3974e-23, 1.8672e-22, 2.7282e-35, 1.0000e+00,\n",
       "         2.9986e-12, 1.0901e-29, 2.0097e-24, 5.1073e-16],\n",
       "        [3.4550e-34, 1.7205e-37, 4.9422e-22, 9.5420e-36, 8.9511e-20, 5.9180e-02,\n",
       "         0.0000e+00, 1.5161e-16, 1.0524e-19, 9.4082e-01],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3479e-27, 9.0324e-37, 7.3584e-02,\n",
       "         0.0000e+00, 9.2642e-01, 5.1884e-20, 3.8045e-14],\n",
       "        [1.2440e-36, 8.6489e-32, 3.8326e-14, 2.7307e-28, 9.9885e-01, 0.0000e+00,\n",
       "         1.1514e-03, 0.0000e+00, 1.2517e-16, 0.0000e+00],\n",
       "        [6.7967e-35, 0.0000e+00, 1.1199e-20, 0.0000e+00, 1.1258e-34, 8.7844e-20,\n",
       "         5.6602e-37, 5.2825e-30, 1.0000e+00, 0.0000e+00],\n",
       "        [6.5722e-20, 1.9952e-09, 0.0000e+00, 1.0000e+00, 2.8868e-22, 0.0000e+00,\n",
       "         6.4331e-21, 0.0000e+00, 1.4939e-27, 5.9703e-28],\n",
       "        [3.4753e-17, 5.6574e-21, 1.0000e+00, 2.2051e-19, 1.3151e-13, 4.3303e-24,\n",
       "         1.4768e-11, 0.0000e+00, 5.4763e-36, 1.4705e-23],\n",
       "        [7.1467e-31, 2.9441e-23, 1.4710e-05, 1.0797e-25, 9.9999e-01, 0.0000e+00,\n",
       "         1.9641e-08, 1.5884e-37, 5.6001e-27, 0.0000e+00],\n",
       "        [3.9811e-04, 9.8899e-01, 3.1469e-07, 9.8024e-03, 3.0581e-05, 4.2333e-15,\n",
       "         7.7900e-04, 8.0822e-13, 8.0365e-13, 8.3447e-11],\n",
       "        [1.0000e+00, 1.5936e-28, 8.8574e-21, 4.3008e-10, 3.6050e-28, 0.0000e+00,\n",
       "         2.8134e-12, 0.0000e+00, 1.3966e-36, 3.3369e-38],\n",
       "        [1.9759e-17, 2.3257e-18, 1.1829e-07, 2.3719e-14, 1.0000e+00, 2.7788e-29,\n",
       "         2.4990e-21, 1.0277e-37, 1.4831e-10, 1.9627e-24],\n",
       "        [2.3827e-19, 4.7281e-15, 4.7839e-30, 1.2893e-25, 9.1585e-23, 0.0000e+00,\n",
       "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [9.8888e-01, 6.4692e-09, 7.9478e-09, 1.1122e-02, 2.0350e-16, 0.0000e+00,\n",
       "         2.9178e-08, 3.3902e-36, 1.3392e-23, 5.5972e-18],\n",
       "        [2.3650e-20, 0.0000e+00, 6.8989e-34, 5.7092e-32, 3.6858e-34, 1.0000e+00,\n",
       "         7.9921e-30, 3.9775e-17, 4.1399e-15, 8.4883e-25],\n",
       "        [0.0000e+00, 0.0000e+00, 1.1517e-37, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 7.0577e-27, 5.7494e-16],\n",
       "        [2.1696e-01, 7.2923e-01, 9.7082e-06, 5.3687e-02, 4.1297e-05, 3.6910e-18,\n",
       "         5.8976e-05, 9.1654e-25, 5.3232e-12, 1.5769e-05],\n",
       "        [1.0000e+00, 1.2388e-27, 1.2139e-22, 7.6834e-15, 2.1325e-23, 0.0000e+00,\n",
       "         1.0186e-12, 0.0000e+00, 0.0000e+00, 3.5319e-30],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7014e-36, 2.3463e-15,\n",
       "         0.0000e+00, 4.0487e-14, 1.7885e-25, 1.0000e+00],\n",
       "        [2.8315e-18, 1.0000e+00, 1.6867e-33, 2.2293e-14, 4.4820e-32, 4.5919e-32,\n",
       "         3.7107e-23, 1.7974e-30, 3.1653e-27, 7.9887e-32],\n",
       "        [2.4295e-17, 0.0000e+00, 7.8860e-16, 7.5500e-19, 5.7458e-10, 1.0607e-09,\n",
       "         6.6516e-06, 5.8748e-10, 9.9999e-01, 6.5099e-19],\n",
       "        [2.4668e-11, 6.7722e-22, 1.0000e+00, 4.8532e-21, 1.4657e-08, 1.0687e-36,\n",
       "         2.3799e-09, 1.3887e-32, 3.7058e-07, 3.4016e-32]], device='mps:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(y_pred, 1) # This is turning the tensor of numbers into probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 10])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(y_pred,1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 9, 1, 4, 9, 0, 0, 9, 9, 7, 4, 5, 1, 2, 9, 4, 4, 4, 5, 5, 5, 9, 7,\n",
       "        4, 8, 3, 2, 4, 1, 0, 4, 6, 0, 5, 5, 1, 0, 9, 1, 8, 2], device='mps:0')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_pred = torch.softmax(y_pred,1).argmax(1) # This tells us the most likely class the network thinks it should be.\n",
    "class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 9, 1, 4, 9, 0, 0, 5, 9, 5, 2, 7, 1, 2, 9, 4, 4, 4, 5, 5, 5, 9, 7,\n",
       "        4, 8, 3, 2, 4, 6, 0, 3, 0, 6, 5, 5, 3, 0, 9, 1, 6, 0], device='mps:0')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7381, device='mps:0')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(class_pred == y_truth).float().mean() # Want this number to be as close to 1 as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RkieTbwlYWPS"
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "  def __init__(self, weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean', label_smoothing=0.0):\n",
    "    super(CrossEntropyLoss, self).__init__()\n",
    "\n",
    "  def forward(self, y_pred, y_truth):\n",
    "    \n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None, init=\"unif\"):\n",
    "    self.__dict__.update(locals()) # keys are arguments we pass in! that way we can just use a dot for any of the argument inputs when we want to reference them, ie self.in_channels\n",
    "    super(Conv2d, self).__init__()\n",
    "\n",
    "    self.weight = Parameter(torch.Tensor(out_channels,\n",
    "                               in_channels, *kernel_size))\n",
    "    self.bias = Parameter(torch.Tensor(out_channels))\n",
    "\n",
    "    self.bias.data.uniform_(0,0)\n",
    "    if init == \"unif\":\n",
    "      self.weight.data.uniform_(-1,1)\n",
    "    if init == \"Xe\":\n",
    "      var = 1 / (in_channels * kernel_size[0] * kernel_size[1])\n",
    "      d = ((12*var)**(.5))/2\n",
    "      d = 6./(1/var + out_channels)\n",
    "      self.weight.data.uniform_(-d,d)\n",
    "    if init == \"orthogonal\":\n",
    "      X = np.random.random((out_channels,in_channels*kernel_size[0]*kernel_size[1]))\n",
    "      U, _, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "      self.weight.data = torch.from_numpy(Vt.reshape((out_channels,in_channels,kernel_size[0],kernel_size[1])).astype(np.float32))\n",
    "\n",
    "    self.weight.data = self.weight.data.to(\"mps\")\n",
    "    self.bias.data = self.bias.data.to(\"mps\")\n",
    "\n",
    "  def forward(self, x):\n",
    "    return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "  # def extra_repr(self) -> str:\n",
    "  #   return super().extra_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 30])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy will produce float64 by default, but we need float32, so to get around this, do something like this:\n",
    "\\begin{align*}\n",
    "    \\verb+torch.from_numpy(np.random.rand(10,10).astype(np.float32))+\n",
    "\\end{align*}\n",
    "and this will make sure the datatypes are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "d4C-_v9Hm7YE"
   },
   "outputs": [],
   "source": [
    "class ConvNetwork(nn.Module):\n",
    "  def __init__(self, dataset, init):\n",
    "    super(ConvNetwork, self).__init__()\n",
    "    x, y = dataset[0]\n",
    "    c, h, w = x.size()\n",
    "    print(c, h, w)\n",
    "    output = 10\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "      Conv2d(c, 10, (3,3), padding=(1,1), init=init),\n",
    "      Conv2d(10, output, (28,28), padding=(0,0), init=init) # 28 x 28 indicates the image size\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x).squeeze(2).squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jYqywck8Wdm9",
    "outputId": "7c006489-be49-42b7-c3ed-0bf4656bae88",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 28 28\n"
     ]
    }
   ],
   "source": [
    "# Initialize device \n",
    "device = \"mps\"\n",
    "# Initialize Datasets\n",
    "train_dataset = FashionMNISTProcessedDataset(\"/tmp/fashionmnist\", train=True)\n",
    "val_dataset = FashionMNISTProcessedDataset(\"tmp/fashionmnist\",train=False)\n",
    "# Initialize DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=42, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=42)\n",
    "# Initialize Model\n",
    "model = ConvNetwork(train_dataset)\n",
    "model = model.to(device)\n",
    "# Initialize Objective and Optimizer and other parameters\n",
    "objective = nn.CrossEntropyLoss() #TODO: create Cross entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# Initialize empty train and validation loss lists\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "# Number of epochs to run through\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.net[0].weight.data.uniform_(-1,1)\n",
    "# model.net[0].weight.data\n",
    "\n",
    "# o, i, k1, k2 = model.net[0].weight.size()\n",
    "# W = np.random.randn(*model.net[0].weight.data.size())\n",
    "\n",
    "# model.net[0].weight.data = torch.from_numpy(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (2830154579.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[151], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    a,b,c,d = *model.net[0].weight.data.size()\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = *model.net[0].weight.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "mTg1jyIsYVZN",
    "outputId": "cfc985dd-006e-4044-dc4b-834a00dcbac1",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/142900 [02:16<?, ?it/s]\n",
      "epoch no.:2 batch no.:428 loss:8.282930374145508 val_loss:4.594259302735828:   0%|          | 0/142900 [00:30<?, ?it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m     val_loss_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1505\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1514\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1512\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1513\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1514\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1515\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/functional.py:3061\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3059\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3060\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3061\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run your training and validation loop and collect stats\n",
    "loop = tqdm(total=len(train_loader) * num_epochs, position=0)\n",
    "for epoch in range(num_epochs):\n",
    "    batch = 0\n",
    "    for x, y_truth in train_loader:\n",
    "        x, y_truth = x.to(device), y_truth.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = objective(y_pred, y_truth)\n",
    "\n",
    "        if epoch % 2 == 0 and batch == 0:\n",
    "            val_loss_list = []\n",
    "            train_loss.append(loss.item())\n",
    "            for val_x, val_y_truth in val_loader:\n",
    "                val_x, val_y_truth = val_x.to(device), val_y_truth.to(device)\n",
    "                val_y_pred = model(val_x)\n",
    "                val_loss_list.append(objective(val_y_pred, val_y_truth).item())\n",
    "            val_loss.append(np.mean(val_loss_list))\n",
    "        \n",
    "        loop.set_description(\"epoch no.:\" + str(epoch) + \" batch no.:\" + str(batch) + \" loss:\" + str(loss.item()) + \" val_loss:\" + str(val_loss[-1]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch += 1\n",
    "\n",
    "loop.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ronkEckHiDaU"
   },
   "source": [
    "\n",
    "___\n",
    "\n",
    "### Part 3\n",
    "Print the number of parameters in your network and plot accuracy of your training and validation \n",
    "set over time. You should experiment with some deep networks and see if you can get a network \n",
    "with close to 1,000,000 parameters.\n",
    "\n",
    "Once you've experimented with multiple network setups and the different initialization strategies, plot the best-performing experiment here. You should be able to exceed 85% accuracy on the validation set.\n",
    "\n",
    "**TODO:**\n",
    "* Experiment with Deep Networks\n",
    "* Plot accuracy of training and validation set over time\n",
    "* Print out number of parameters in the model \n",
    "* Plot experiment results with 85% or better validation accuracy\n",
    "\n",
    "**DONE:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "PaWCKjxvyRSf",
    "outputId": "698e686b-9e1c-4e6e-d4a4-37a29f6f325d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Go back up and try a few different networks and initialization strategies\n",
    "# Plot loss if you want\n",
    "# Plot accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "oijCR-JnyS6V",
    "outputId": "019b290d-7a98-41ac-e6c5-d1df99a7cbf2",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print the number of parameters in the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hXGRxUQh9gX"
   },
   "source": [
    "___\n",
    "\n",
    "### Part 4\n",
    "Learn about how convolution layers affect the shape of outputs, and answer the following quiz questions. Include these in a new markdown cell in your jupyter notebook.\n",
    "\n",
    "\n",
    "*Using a Kernel size of 3×3 what should the settings of your 2d convolution be that results in the following mappings (first answer given to you)*\n",
    "\n",
    "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(3, 3), padding=(0, 0))\n",
    "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=65, h=12, w=12) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=7, h=20, w=20) : **Your answer in bold here**\n",
    "\n",
    "*Using a Kernel size of 5×5:*)\n",
    "\n",
    "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(5, 5), padding=(1, 1))\n",
    "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **Your answer in bold here**\n",
    "\n",
    "*Using Kernel size of 5×3:*\n",
    "\n",
    "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **Your answer in bold here**\n",
    "\n",
    "*Determine the kernel that requires the smallest padding size to make the following mappings possible:*\n",
    "\n",
    "* (c=3, h=10, w=10) ⇒ (c=10, h=9, w=7) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Your answer in bold here**\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Answer all the questions above \n",
    "\n",
    "**DONE:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XXfG3wClh8an",
    "outputId": "a616216f-6637-495e-c596-4fc989677d80",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Write some test code for checking the answers for these problems (example shown in the video)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "DL_Lab3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.17 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "e1cb4ba5f411cfa4a68a7ea6c2f9ba3655e2604bd37447d058a856eda531fd15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
